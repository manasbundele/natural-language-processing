# Text Generation using N-Gram Language Model

This project consists of multiple parts: 
1. Developing a MLE based N-Gram Language Model
2. Adding support for Out-of-Vocabulary Words
3. Adding Laplace smoothing, Katz Backoff smoothing and Linear Interpolation smoothing
4. Author Identification using a trained language model
5. Text generation based on a trained language model

### Select few results

Few generated sentences of max-length 10 based on shakespeare.txt

* Likeliest sentence for bigram: And I am a <unk>

* Likeliest sentence for trigram: And I will not be <unk>

* Likeliest sentence for 4-gram: And I will take thee in the

* Likeliest sentence for 5-gram: And I will take thy word: yet if thou swear'st,

